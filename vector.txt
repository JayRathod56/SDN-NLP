import math
from collections import defaultdict

def word_tokenize(sentence):
    words = []
    current_word = ""
    for char in sentence:
        if char.isalpha():
            current_word += char.lower()
        elif current_word:
            words.append(current_word)
            current_word = ""
    if current_word:
        words.append(current_word)
    return words

def remove_stopwords(tokens, stopwords):
    filtered_tokens = []
    for token in tokens:
        if token not in stopwords:
            filtered_tokens.append(token)
    return filtered_tokens

def lemmatize_tokens(tokens):
    lemmatized_tokens = []
    for token in tokens:
        lemmatized_tokens.append(token.lower())
    return lemmatized_tokens

def calculate_word_frequencies(tokens):
    freq_dist = defaultdict(int)
    for token in tokens:
        freq_dist[token] += 1
    return freq_dist

def calculate_word_probability(sentence, word):
    tokens = word_tokenize(sentence)
    total_words = len(tokens)

    freq_dist = calculate_word_frequencies(tokens)
    word_freq = freq_dist[word]

    probability = word_freq / total_words
    return probability

def generate_word_embeddings(sentence):
    tokens = word_tokenize(sentence)
    lemmatized_tokens = lemmatize_tokens(tokens)

    word_embeddings = {}
    for i, token in enumerate(lemmatized_tokens):
        if token not in word_embeddings:
            word_embeddings[token] = [0] * len(lemmatized_tokens)
        word_embeddings[token][i] += 1

    return word_embeddings

def calculate_cosine_similarity(embeddings, word1, word2):
    vec1 = embeddings[word1]
    vec2 = embeddings[word2]

    dot_product = sum(v1 * v2 for v1, v2 in zip(vec1, vec2))
    norm1 = math.sqrt(sum(v * v for v in vec1))
    norm2 = math.sqrt(sum(v * v for v in vec2))

    similarity = dot_product / (norm1 * norm2)
    return similarity

def update_word_embeddings(embeddings, sentence):
    tokens = word_tokenize(sentence)
    lemmatized_tokens = lemmatize_tokens(tokens)

    for i, token in enumerate(lemmatized_tokens):
        if token not in embeddings:
            embeddings[token] = [0] * len(lemmatized_tokens)
        embeddings[token][i] += 1

    return embeddings

# Example usage
sentence = "I love eating pizza and burgers."
word = "pizza"

# Obtain initial word embeddings
initial_embeddings = generate_word_embeddings(sentence)
print(f"Initial Embedding for '{word}': {initial_embeddings[word]}")

# Calculate word probability
probability = calculate_word_probability(sentence, word)
print(f"Probability of '{word}' occurring: {probability}")

# Update the sentence
updated_sentence = "I love eating pizza and pasta."

# Obtain updated word embeddings
updated_embeddings = update_word_embeddings(initial_embeddings, updated_sentence)
print(f"\nUpdated Embedding for '{word}': {updated_embeddings[word]}")

----------------------------------------------------------------------------------------------------


sentence = "The Quick brown fox jumps over the lazy dog"

sentence2 = "Consider an nlp's brown corpus and skipgram based task that requires brian"

target_word = "fox"

def sentence_to_list(sent):
  lit = sent.split()

  return lit

lit = sentence_to_list(sentence)

def list_to_dic_with_embedings(lit):

  dic = {}

  import random

  for j in range(len(lit)):
    embiding = []
    for i in range(4):

      embiding.append(random.randint(0,10))

      import numpy as np
      arr = np.array(embiding)

    dic = {**dic, lit[j]: arr}

  return dic

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity



# Dynamic word embiddings
word_embeddings = list_to_dic_with_embedings(lit)

print("\n\nWord Embeddings")
print(word_embeddings)

# Split sentences into words
words1 = sentence.split()
print(words1)
words2 = sentence2.split()

# Find the target word (e.g., the first word in sentence1)
print("\ntarget word : ", target_word)

# Calculate embeddings for each word in sentence1 and sentence2
embeddings1 = [word_embeddings.get(word, np.zeros(4)) for word in words1]
embeddings2 = [word_embeddings.get(word, np.zeros(4)) for word in words2]

print("\n\nEmbeddings 1 and 2")

print(embeddings1)
print(embeddings2)

print("\n\n\n Initial Embeddings")
print(embeddings1,embeddings2)
# Calculate cosine similarity between target word embedding and all context word embeddings
target_embedding = word_embeddings.get(target_word, np.zeros(4))
similarities = cosine_similarity([target_embedding], embeddings2)[0]

# Sample positive and negative context probabilities
p_positive = 0.8  # Probability of positive context
p_negative = [0.2, 0.3, 0.1, 0.4]  # Probabilities of negative contexts

# Apply LCE formula to adjust the embeddings
lce_adjusted_embeddings = []
for similarity, context_embedding, p_neg in zip(similarities, embeddings2, p_negative):
    p_pos = p_positive
    lce = -np.log(p_pos) + np.sum([-np.log(p_neg)])
    lce_adjusted_embedding = target_embedding + lce * context_embedding
    lce_adjusted_embeddings.append(lce_adjusted_embedding)

print("\n\n\nUpdated Embeddings")
print(lce_adjusted_embeddings)
print("Similarity: ",similarity)
